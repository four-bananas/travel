<configuration>
  <!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 -->
  <property>
    <name>dfs.nameservices</name>
    <value>ns1</value>
  </property>
  <!-- ns1下面有两个NameNode，分别是nn1，nn2 -->
  <property>
    <name>dfs.ha.namenodes.ns1</name>
    <value>nn1,nn2</value>
  </property>
  <!-- nn1的RPC通信地址 -->
  <property>
    <name>dfs.namenode.rpc-address.ns1.nn1</name>
    <value>spark01:9000</value>
  </property>
  <!-- nn1的http通信地址 -->
  <property>
    <name>dfs.namenode.http-address.ns1.nn1</name>
    <value>spark01:50070</value>
  </property>
  <!-- nn2的RPC通信地址 -->
  <property>
    <name>dfs.namenode.rpc-address.ns1.nn2</name>
    <value>spark02:9000</value>
  </property>
  <!-- nn2的http通信地址 -->
  <property>
    <name>dfs.namenode.http-address.ns1.nn2</name>
    <value>spark02:50070</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>spark01:50090</value>
  </property>
  <property>
    <name>dfs.namenode.http-address</name>
    <value>spark01:50070</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///hadoopadmin/datadir/hadoop/namenodeDatas</value>
  </property>
  <!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割  -->
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///hadoopadmin/datadir/hadoop/datanodeDatas</value>
  </property>
  <property>
    <name>dfs.namenode.edits.dir</name>
    <value>file:///hadoopadmin/datadir/hadoop/dfs/nn/edits</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.dir</name>
    <value>file:///hadoopadmin/datadir/hadoop/dfs/snn/name</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.edits.dir</name>
    <value>file:///hadoopadmin/datadir/hadoop/dfs/nn/snn/edits</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.blocksize</name>
    <value>134217728</value>
  </property>
  <!-- 指定NameNode的元数据在JournalNode上的存放位置 -->
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://spark01:8485;spark02:8485;spark03:8485/ns1</value>
  </property>
  <!-- 指定JournalNode在本地磁盘存放数据的位置 -->
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/hadoopadmin/datadir/hadoop/journal</value>
  </property>
  <!-- 开启NameNode失败自动切换 -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
  <!-- 配置失败自动切换实现方式 -->
  <property>
    <name>dfs.client.failover.proxy.provider.ns1</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>
      sshfence
      shell(/bin/true)
    </value>
  </property>
  <!-- 使用sshfence隔离机制时需要ssh免登陆 -->
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/home/hadoopadmin/.ssh/id_rsa</value>
  </property>
  <!-- 配置sshfence隔离机制超时时间 -->
  <property>
    <name>dfs.ha.fencing.ssh.connect-timeout</name>
    <value>30000</value>
  </property>
</configuration>